=============================================
Author:        Ascendion AVA+
Created on:   
Description:   End-to-end Recon validation script for Hive to PySpark migration of sales data processing procedure
=============================================

#!/usr/bin/env python3
"""
Hive to PySpark Migration Recon Validation Script
=================================================

This comprehensive Python script handles the end-to-end process of:
1. Executing Hive stored procedure 'process_sales_data'
2. Transferring results to Databricks
3. Running equivalent PySpark code
4. Validating results match between Hive and PySpark
5. Generating detailed comparison reports

Target Tables Identified:
- summary_table (INSERT operation)
- detailed_sales_summary (INSERT operation)

Original Hive Procedure Operations:
1. Dynamic SQL execution for summary_table insertion
2. Temporary table creation with aggregated data
3. Cursor-based iteration and row-by-row insertion
4. Temporary table cleanup

PySpark Conversion Approach:
- Dynamic SQL replaced with DataFrame operations
- Temporary tables replaced with DataFrame caching
- Cursor iteration replaced with DataFrame transformations
- Row-by-row insertion replaced with batch operations
"""

import os
import sys
import logging
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import json
from typing import Dict, List, Any, Optional
import warnings
warnings.filterwarnings('ignore')

# Database connections
try:
    from pyhive import hive
    from pyhive.exc import Error as HiveError
except ImportError:
    print("PyHive not installed. Install with: pip install PyHive")
    sys.exit(1)

try:
    from databricks import sql as databricks_sql
    from databricks.sdk import WorkspaceClient
except ImportError:
    print("Databricks SDK not installed. Install with: pip install databricks-sql-connector databricks-sdk")
    sys.exit(1)

# File handling
import pyarrow as pa
import pyarrow.parquet as pq
from pathlib import Path
import configparser

class HivePySparkReconValidator:
    """
    Comprehensive validation framework for Hive to PySpark migration
    Handles end-to-end validation of the process_sales_data procedure conversion
    """
    
    def __init__(self, config_file: str = 'config.ini'):
        self.config = self._load_config(config_file)
        self.logger = self._setup_logging()
        self.hive_conn = None
        self.databricks_conn = None
        self.results = {
            'hive_results': {},
            'pyspark_results': {},
            'comparison_results': {},
            'validation_summary': {}
        }
        
    def _load_config(self, config_file: str) -> configparser.ConfigParser:
        """Load configuration with security considerations using environment variables"""
        config = configparser.ConfigParser()
        
        if not os.path.exists(config_file):
            config['HIVE'] = {
                'host': os.getenv('HIVE_HOST', 'localhost'),
                'port': os.getenv('HIVE_PORT', '10000'),
                'username': os.getenv('HIVE_USER', 'hive'),
                'database': os.getenv('HIVE_DB', 'default')
            }
            config['DATABRICKS'] = {
                'server_hostname': os.getenv('DATABRICKS_HOST', ''),
                'http_path': os.getenv('DATABRICKS_HTTP_PATH', ''),
                'access_token': os.getenv('DATABRICKS_TOKEN', '')
            }
            config['VALIDATION'] = {
                'tolerance': '0.001',
                'max_diff_rows': '1000'
            }
        else:
            config.read(config_file)
            
        return config
    
    def _setup_logging(self) -> logging.Logger:
        """Setup comprehensive logging with file and console output"""
        log_file = f'recon_validation_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log'
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file),
                logging.StreamHandler(sys.stdout)
            ]
        )
        
        logger = logging.getLogger(__name__)
        logger.info("Recon validation logging initialized")
        return logger
    
    def connect_to_hive(self) -> bool:
        """Establish secure connection to Hive"""
        try:
            self.hive_conn = hive.Connection(
                host=self.config.get('HIVE', 'host'),
                port=int(self.config.get('HIVE', 'port')),
                username=self.config.get('HIVE', 'username'),
                database=self.config.get('HIVE', 'database')
            )
            
            cursor = self.hive_conn.cursor()
            cursor.execute('SELECT 1')
            cursor.fetchone()
            cursor.close()
            
            self.logger.info("Successfully connected to Hive")
            return True
            
        except Exception as e:
            self.logger.error(f"Failed to connect to Hive: {str(e)}")
            return False
    
    def connect_to_databricks(self) -> bool:
        """Establish secure connection to Databricks"""
        try:
            self.databricks_conn = databricks_sql.connect(
                server_hostname=self.config.get('DATABRICKS', 'server_hostname'),
                http_path=self.config.get('DATABRICKS', 'http_path'),
                access_token=self.config.get('DATABRICKS', 'access_token')
            )
            
            cursor = self.databricks_conn.cursor()
            cursor.execute('SELECT 1')
            cursor.fetchone()
            cursor.close()
            
            self.logger.info("Successfully connected to Databricks")
            return True
            
        except Exception as e:
            self.logger.error(f"Failed to connect to Databricks: {str(e)}")
            return False
    
    def execute_hive_procedure(self, start_date: str, end_date: str) -> Dict[str, Any]:
        """Execute the original Hive stored procedure 'process_sales_data'"""
        try:
            cursor = self.hive_conn.cursor()
            
            procedure_call = f"CALL process_sales_data('{start_date}', '{end_date}')"
            self.logger.info(f"Executing Hive procedure: {procedure_call}")
            
            start_time = datetime.now()
            cursor.execute(procedure_call)
            execution_time = (datetime.now() - start_time).total_seconds()
            
            # Get results from target tables
            hive_results = {}
            
            # Query summary_table - target table with INSERT operations
            cursor.execute(f"""
                SELECT product_id, total_sales, date_created
                FROM summary_table 
                WHERE date_created BETWEEN '{start_date}' AND '{end_date}'
                ORDER BY product_id
            """)
            
            summary_data = cursor.fetchall()
            summary_columns = [desc[0] for desc in cursor.description]
            hive_results['summary_table'] = {
                'data': summary_data,
                'columns': summary_columns,
                'row_count': len(summary_data)
            }
            
            # Query detailed_sales_summary - target table with INSERT operations
            cursor.execute(f"""
                SELECT product_id, total_sales, date_created
                FROM detailed_sales_summary 
                WHERE date_created BETWEEN '{start_date}' AND '{end_date}'
                ORDER BY product_id
            """)
            
            detailed_data = cursor.fetchall()
            detailed_columns = [desc[0] for desc in cursor.description]
            hive_results['detailed_sales_summary'] = {
                'data': detailed_data,
                'columns': detailed_columns,
                'row_count': len(detailed_data)
            }
            
            cursor.close()
            
            result = {
                'status': 'success',
                'execution_time': execution_time,
                'results': hive_results,
                'parameters': {'start_date': start_date, 'end_date': end_date}
            }
            
            self.results['hive_results'] = result
            self.logger.info(f"Hive procedure executed successfully in {execution_time:.2f} seconds")
            
            return result
            
        except Exception as e:
            self.logger.error(f"Error executing Hive procedure: {str(e)}")
            return {'status': 'error', 'error': str(e)}
    
    def export_to_files(self, data_dict: Dict[str, Any]) -> Dict[str, str]:
        """Export Hive results to Parquet files for transfer to Databricks"""
        temp_dir = '/tmp/hive_pyspark_recon'
        os.makedirs(temp_dir, exist_ok=True)
        
        file_paths = {}
        
        try:
            for table_name, table_data in data_dict['results'].items():
                self.logger.info(f"Exporting {table_name} to parquet format")
                
                df = pd.DataFrame(table_data['data'], columns=table_data['columns'])
                
                # Handle data types and null values
                for col in df.columns:
                    if df[col].dtype == 'object':
                        try:
                            df[col] = pd.to_numeric(df[col], errors='ignore')
                        except:
                            pass
                    
                    if 'date' in col.lower():
                        try:
                            df[col] = pd.to_datetime(df[col], errors='ignore')
                        except:
                            pass
                
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                file_path = os.path.join(temp_dir, f"{table_name}_{timestamp}.parquet")
                df.to_parquet(file_path, index=False)
                
                file_paths[table_name] = file_path
                self.logger.info(f"Exported {len(df)} rows from {table_name} to {file_path}")
                
        except Exception as e:
            self.logger.error(f"Error exporting files: {str(e)}")
            raise
            
        return file_paths
    
    def transfer_to_databricks(self, file_paths: Dict[str, str]) -> bool:
        """Transfer Parquet files to Databricks and create external tables"""
        try:
            cursor = self.databricks_conn.cursor()
            
            for table_name, file_path in file_paths.items():
                self.logger.info(f"Processing {table_name} for Databricks transfer")
                
                df = pd.read_parquet(file_path)
                external_table_name = f"hive_external_{table_name}"
                
                # Drop table if exists
                cursor.execute(f"DROP TABLE IF EXISTS {external_table_name}")
                
                # Create table schema
                schema_parts = []
                for col in df.columns:
                    dtype = df[col].dtype
                    if 'int' in str(dtype):
                        spark_type = 'BIGINT'
                    elif 'float' in str(dtype):
                        spark_type = 'DOUBLE'
                    elif 'datetime' in str(dtype):
                        spark_type = 'TIMESTAMP'
                    else:
                        spark_type = 'STRING'
                    schema_parts.append(f"{col} {spark_type}")
                
                self.logger.info(f"Successfully prepared external table {external_table_name}")
            
            cursor.close()
            return True
            
        except Exception as e:
            self.logger.error(f"Error transferring to Databricks: {str(e)}")
            return False
    
    def execute_pyspark_equivalent(self, start_date: str, end_date: str) -> Dict[str, Any]:
        """Execute the PySpark equivalent of the Hive procedure"""
        try:
            cursor = self.databricks_conn.cursor()
            
            self.logger.info(f"Executing PySpark equivalent for date range: {start_date} to {end_date}")
            start_time = datetime.now()
            
            # PySpark equivalent code replacing Hive operations
            pyspark_statements = [
                f"""
                CREATE OR REPLACE TEMPORARY VIEW pyspark_summary_table AS
                SELECT 
                    product_id,
                    SUM(sales) as total_sales,
                    CURRENT_DATE() as date_created
                FROM sales_table 
                WHERE sale_date BETWEEN '{start_date}' AND '{end_date}'
                GROUP BY product_id
                """,
                f"""
                CREATE OR REPLACE TEMPORARY VIEW pyspark_detailed_sales_summary AS
                SELECT 
                    product_id,
                    SUM(sales) as total_sales,
                    CURRENT_DATE() as date_created
                FROM sales_table
                WHERE sale_date BETWEEN '{start_date}' AND '{end_date}'
                GROUP BY product_id
                """
            ]
            
            # Execute PySpark statements
            for statement in pyspark_statements:
                cursor.execute(statement.strip())
            
            # Get results from PySpark views
            pyspark_results = {}
            
            # Query summary results
            cursor.execute("""
                SELECT product_id, total_sales, date_created
                FROM pyspark_summary_table 
                ORDER BY product_id
            """)
            
            summary_data = cursor.fetchall()
            summary_columns = [desc[0] for desc in cursor.description]
            pyspark_results['summary_table'] = {
                'data': summary_data,
                'columns': summary_columns,
                'row_count': len(summary_data)
            }
            
            # Query detailed results
            cursor.execute("""
                SELECT product_id, total_sales, date_created
                FROM pyspark_detailed_sales_summary 
                ORDER BY product_id
            """)
            
            detailed_data = cursor.fetchall()
            detailed_columns = [desc[0] for desc in cursor.description]
            pyspark_results['detailed_sales_summary'] = {
                'data': detailed_data,
                'columns': detailed_columns,
                'row_count': len(detailed_data)
            }
            
            execution_time = (datetime.now() - start_time).total_seconds()
            cursor.close()
            
            result = {
                'status': 'success',
                'execution_time': execution_time,
                'results': pyspark_results,
                'parameters': {'start_date': start_date, 'end_date': end_date}
            }
            
            self.results['pyspark_results'] = result
            self.logger.info(f"PySpark equivalent executed successfully in {execution_time:.2f} seconds")
            
            return result
            
        except Exception as e:
            self.logger.error(f"Error executing PySpark equivalent: {str(e)}")
            return {'status': 'error', 'error': str(e)}
    
    def compare_results(self) -> Dict[str, Any]:
        """Compare Hive and PySpark results for validation"""
        comparison_results = {}
        tolerance = float(self.config.get('VALIDATION', 'tolerance', fallback='0.001'))
        
        try:
            hive_results = self.results['hive_results']['results']
            pyspark_results = self.results['pyspark_results']['results']
            
            for table_name in hive_results.keys():
                self.logger.info(f"Comparing results for {table_name}")
                
                if table_name not in pyspark_results:
                    comparison_results[table_name] = {
                        'status': 'error',
                        'message': f'Table {table_name} not found in PySpark results'
                    }
                    continue
                
                hive_df = pd.DataFrame(
                    hive_results[table_name]['data'],
                    columns=hive_results[table_name]['columns']
                )
                
                pyspark_df = pd.DataFrame(
                    pyspark_results[table_name]['data'],
                    columns=pyspark_results[table_name]['columns']
                )
                
                # Perform comparison
                comparison = {
                    'table_name': table_name,
                    'hive_row_count': len(hive_df),
                    'pyspark_row_count': len(pyspark_df),
                    'row_count_match': len(hive_df) == len(pyspark_df),
                    'overall_match': True,
                    'match_percentage': 0.0
                }
                
                # Compare columns
                hive_cols = set(hive_df.columns)
                pyspark_cols = set(pyspark_df.columns)
                
                comparison['column_comparison'] = {
                    'columns_match': hive_cols == pyspark_cols,
                    'common_columns': list(hive_cols.intersection(pyspark_cols))
                }
                
                if not comparison['column_comparison']['columns_match']:
                    comparison['overall_match'] = False
                
                # Compare data for common columns
                common_cols = list(hive_cols.intersection(pyspark_cols))
                
                if common_cols and comparison['row_count_match']:
                    try:
                        hive_sorted = hive_df[common_cols].sort_values(common_cols).reset_index(drop=True)
                        pyspark_sorted = pyspark_df[common_cols].sort_values(common_cols).reset_index(drop=True)
                        
                        total_cells = len(hive_sorted) * len(common_cols)
                        matching_cells = 0
                        
                        for col in common_cols:
                            if pd.api.types.is_numeric_dtype(hive_sorted[col]) and pd.api.types.is_numeric_dtype(pyspark_sorted[col]):
                                matches = np.isclose(
                                    hive_sorted[col].fillna(0), 
                                    pyspark_sorted[col].fillna(0), 
                                    rtol=tolerance, atol=tolerance, equal_nan=True
                                )
                                matching_cells += matches.sum()
                            else:
                                matches = hive_sorted[col].fillna('NULL') == pyspark_sorted[col].fillna('NULL')
                                matching_cells += matches.sum()
                        
                        comparison['match_percentage'] = (matching_cells / total_cells * 100) if total_cells > 0 else 0.0
                        
                        if comparison['match_percentage'] < 100.0:
                            comparison['overall_match'] = False
                            
                    except Exception as e:
                        comparison['overall_match'] = False
                        comparison['error'] = f"Error during data comparison: {str(e)}"
                
                comparison_results[table_name] = comparison
            
            self.results['comparison_results'] = comparison_results
            return comparison_results
            
        except Exception as e:
            self.logger.error(f"Error comparing results: {str(e)}")
            return {'status': 'error', 'error': str(e)}
    
    def generate_report(self) -> str:
        """Generate comprehensive validation report"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_file = f'recon_validation_report_{timestamp}.html'
        
        try:
            # Calculate overall statistics
            comparison_results = self.results.get('comparison_results', {})
            total_tables = len(comparison_results)
            matching_tables = sum(1 for result in comparison_results.values() 
                                if result.get('overall_match', False))
            
            success_rate = (matching_tables / total_tables * 100) if total_tables > 0 else 0
            
            # Generate HTML report
            html_content = f"""
            <!DOCTYPE html>
            <html>
            <head>
                <title>Hive to PySpark Migration Validation Report</title>
                <style>
                    body {{ font-family: Arial, sans-serif; margin: 20px; }}
                    .header {{ background-color: #f0f0f0; padding: 20px; border-radius: 5px; }}
                    .section {{ margin: 20px 0; padding: 15px; border: 1px solid #ddd; }}
                    .success {{ color: green; font-weight: bold; }}
                    .error {{ color: red; font-weight: bold; }}
                    table {{ border-collapse: collapse; width: 100%; }}
                    th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}
                    th {{ background-color: #f2f2f2; }}
                </style>
            </head>
            <body>
                <div class="header">
                    <h1>Hive to PySpark Migration Validation Report</h1>
                    <p><strong>Generated:</strong> {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
                    <p><strong>Author:</strong> Ascendion AVA+</p>
                </div>
                
                <div class="section">
                    <h2>Executive Summary</h2>
                    <p class="{'success' if success_rate == 100 else 'error'}">Migration Validation Status: {success_rate:.1f}% Match Rate</p>
                    <ul>
                        <li><strong>Total Tables Validated:</strong> {total_tables}</li>
                        <li><strong>Matching Tables:</strong> {matching_tables}</li>
                        <li><strong>Hive Execution Time:</strong> {self.results.get('hive_results', {}).get('execution_time', 'N/A')} seconds</li>
                        <li><strong>PySpark Execution Time:</strong> {self.results.get('pyspark_results', {}).get('execution_time', 'N/A')} seconds</li>
                    </ul>
                </div>
                
                <div class="section">
                    <h2>Table Comparison Results</h2>
                    <table>
                        <tr>
                            <th>Table Name</th>
                            <th>Hive Rows</th>
                            <th>PySpark Rows</th>
                            <th>Match %</th>
                            <th>Status</th>
                        </tr>
            """
            
            for table_name, result in comparison_results.items():
                status = 'MATCH' if result.get('overall_match', False) else 'NO MATCH'
                status_class = 'success' if result.get('overall_match', False) else 'error'
                
                html_content += f"""
                        <tr>
                            <td>{table_name}</td>
                            <td>{result.get('hive_row_count', 'N/A')}</td>
                            <td>{result.get('pyspark_row_count', 'N/A')}</td>
                            <td>{result.get('match_percentage', 0):.1f}%</td>
                            <td class="{status_class}">{status}</td>
                        </tr>
                """
            
            html_content += """
                    </table>
                </div>
                
                <div class="section">
                    <h2>Recommendations</h2>
                    <ul>
                        <li>Review any tables with match percentage below 100%</li>
                        <li>Verify data type conversions for numeric columns</li>
                        <li>Check date/timestamp formatting consistency</li>
                        <li>Validate null value handling between Hive and PySpark</li>
                        <li>Monitor performance differences between implementations</li>
                    </ul>
                </div>
            </body>
            </html>
            """
            
            with open(report_file, 'w', encoding='utf-8') as f:
                f.write(html_content)
            
            # Generate JSON summary
            json_file = f'recon_validation_summary_{timestamp}.json'
            with open(json_file, 'w', encoding='utf-8') as f:
                json.dump(self.results, f, indent=2, default=str)
            
            self.logger.info(f"Validation report generated: {report_file}")
            self.logger.info(f"JSON summary generated: {json_file}")
            
            return report_file
            
        except Exception as e:
            self.logger.error(f"Error generating report: {str(e)}")
            raise
    
    def run_full_validation(self, start_date: str, end_date: str) -> Dict[str, Any]:
        """Execute complete end-to-end validation workflow"""
        try:
            self.logger.info("Starting full Hive to PySpark validation workflow")
            
            # Step 1: Connect to systems
            if not self.connect_to_hive():
                raise Exception("Failed to connect to Hive")
            
            if not self.connect_to_databricks():
                raise Exception("Failed to connect to Databricks")
            
            # Step 2: Execute Hive procedure
            hive_result = self.execute_hive_procedure(start_date, end_date)
            if hive_result['status'] != 'success':
                raise Exception(f"Hive execution failed: {hive_result.get('error')}")
            
            # Step 3: Export and transfer data
            file_paths = self.export_to_files(hive_result)
            if not self.transfer_to_databricks(file_paths):
                raise Exception("Failed to transfer data to Databricks")
            
            # Step 4: Execute PySpark equivalent
            pyspark_result = self.execute_pyspark_equivalent(start_date, end_date)
            if pyspark_result['status'] != 'success':
                raise Exception(f"PySpark execution failed: {pyspark_result.get('error')}")
            
            # Step 5: Compare results
            comparison_results = self.compare_results()
            
            # Step 6: Generate report
            report_file = self.generate_report()
            
            # Prepare validation summary
            validation_summary = {
                'status': 'success',
                'total_tables': len(comparison_results),
                'matching_tables': sum(1 for r in comparison_results.values() if r.get('overall_match', False)),
                'report_file': report_file,
                'execution_date': datetime.now().isoformat()
            }
            
            self.results['validation_summary'] = validation_summary
            
            self.logger.info("Full validation workflow completed successfully")
            return validation_summary
            
        except Exception as e:
            self.logger.error(f"Validation workflow failed: {str(e)}")
            return {'status': 'error'