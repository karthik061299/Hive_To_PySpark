=============================================
Author:        Ascendion AVA+
Created on:   
Description:   End-to-end Hive to PySpark migration validation script for sales data processing
=============================================

# HIVE TO PYSPARK MIGRATION VALIDATION SCRIPT

## OVERVIEW
This comprehensive Python script handles the end-to-end process of executing Hive code, transferring results to Databricks, running equivalent PySpark code, and validating that results match.

## TARGET TABLES IDENTIFIED
- summary_table (INSERT operations from dynamic query)
- detailed_sales_summary (INSERT operations from cursor loop)

## VALIDATION WORKFLOW

### STEP 1: HIVE EXECUTION
```python
import os
import sys
import logging
import pandas as pd
from pyhive import hive
from pyspark.sql import SparkSession
from datetime import datetime
import json

class HivePySparkReconValidator:
    def __init__(self):
        self.hive_conn = None
        self.spark = None
        self.target_tables = ['summary_table', 'detailed_sales_summary']
        self.test_start_date = '2023-01-01'
        self.test_end_date = '2023-12-31'
        self.logger = self._setup_logging()
    
    def connect_to_hive(self):
        """Establish secure connection to Hive"""
        try:
            self.hive_conn = hive.Connection(
                host=os.getenv('HIVE_HOST', 'localhost'),
                port=int(os.getenv('HIVE_PORT', '10000')),
                database=os.getenv('HIVE_DATABASE', 'default'),
                username=os.getenv('HIVE_USERNAME', 'hive'),
                password=os.getenv('HIVE_PASSWORD', ''),
                auth='PLAIN'
            )
            return True
        except Exception as e:
            self.logger.error(f'Hive connection failed: {e}')
            return False
```

### STEP 2: EXECUTE HIVE STORED PROCEDURE
```python
    def execute_hive_procedure(self):
        """Execute the original Hive stored procedure"""
        try:
            cursor = self.hive_conn.cursor()
            
            # Create the stored procedure
            create_proc_sql = '''
            CREATE OR REPLACE PROCEDURE process_sales_data(
                IN start_date STRING,
                IN end_date STRING
            )
            BEGIN
                DECLARE total_sales FLOAT;
                
                SET @dynamic_query = CONCAT(
                    "INSERT INTO summary_table SELECT product_id, SUM(sales) AS total_sales ",
                    "FROM sales_table WHERE sale_date BETWEEN '", start_date, "' AND '", end_date, "' ",
                    "GROUP BY product_id"
                );
                EXECUTE IMMEDIATE @dynamic_query;
                
                CREATE TEMPORARY TABLE temp_sales_summary AS
                SELECT product_id, SUM(sales) AS total_sales
                FROM sales_table
                WHERE sale_date BETWEEN start_date AND end_date
                GROUP BY product_id;
                
                DECLARE cur CURSOR FOR SELECT product_id, total_sales FROM temp_sales_summary;
                
                OPEN cur;
                FETCH cur INTO product_id, total_sales;
                WHILE total_sales IS NOT NULL DO
                    INSERT INTO detailed_sales_summary (product_id, total_sales)
                    VALUES (product_id, total_sales);
                    FETCH cur INTO product_id, total_sales;
                END WHILE;
                
                CLOSE cur;
                DROP TABLE temp_sales_summary;
            END;
            '''
            
            cursor.execute(create_proc_sql)
            
            # Execute the procedure
            execute_sql = f"CALL process_sales_data('{self.test_start_date}', '{self.test_end_date}')"
            cursor.execute(execute_sql)
            
            cursor.close()
            self.logger.info('Hive procedure executed successfully')
            return True
            
        except Exception as e:
            self.logger.error(f'Hive procedure execution failed: {e}')
            return False
```

### STEP 3: EXPORT HIVE TABLES TO PARQUET
```python
    def export_hive_tables_to_parquet(self):
        """Export target tables to Parquet format"""
        parquet_files = {}
        
        try:
            cursor = self.hive_conn.cursor()
            
            for table_name in self.target_tables:
                # Query table data
                cursor.execute(f'SELECT * FROM {table_name}')
                
                # Get column names
                columns = [desc[0] for desc in cursor.description]
                
                # Fetch all data
                data = cursor.fetchall()
                
                # Convert to DataFrame
                df = pd.DataFrame(data, columns=columns)
                
                # Export to Parquet
                parquet_path = f'./exports/{table_name}.parquet'
                df.to_parquet(parquet_path, index=False)
                
                parquet_files[table_name] = parquet_path
                
                self.logger.info(f'Exported {len(df)} rows from {table_name}')
            
            cursor.close()
            return parquet_files
            
        except Exception as e:
            self.logger.error(f'Export failed: {e}')
            return {}
```

### STEP 4: UPLOAD TO DATABRICKS STORAGE
```python
    def upload_to_databricks_storage(self, parquet_files):
        """Upload Parquet files to Databricks storage"""
        uploaded_paths = {}
        
        try:
            # Azure Blob Storage upload
            from azure.storage.blob import BlobServiceClient
            
            blob_service = BlobServiceClient(
                account_url=f"https://{os.getenv('AZURE_ACCOUNT')}.blob.core.windows.net",
                credential=os.getenv('AZURE_KEY')
            )
            
            container_name = os.getenv('AZURE_CONTAINER', 'migration-data')
            
            for table_name, local_path in parquet_files.items():
                blob_name = f'hive_migration/{table_name}.parquet'
                
                with open(local_path, 'rb') as data:
                    blob_service.get_blob_client(
                        container=container_name,
                        blob=blob_name
                    ).upload_blob(data, overwrite=True)
                
                uploaded_paths[table_name] = f'abfss://{container_name}@{os.getenv("AZURE_ACCOUNT")}.dfs.core.windows.net/{blob_name}'
                
                self.logger.info(f'Uploaded {table_name} to Azure Blob Storage')
            
            return uploaded_paths
            
        except Exception as e:
            self.logger.error(f'Upload failed: {e}')
            return {}
```

### STEP 5: INITIALIZE PYSPARK SESSION
```python
    def initialize_spark_session(self):
        """Initialize Spark session with Delta Lake support"""
        try:
            self.spark = SparkSession.builder \
                .appName('HivePySparkMigrationValidation') \
                .config('spark.sql.extensions', 'io.delta.sql.DeltaSparkSessionExtension') \
                .config('spark.sql.catalog.spark_catalog', 'org.apache.spark.sql.delta.catalog.DeltaCatalog') \
                .config('spark.sql.adaptive.enabled', 'true') \
                .config('spark.sql.adaptive.coalescePartitions.enabled', 'true') \
                .getOrCreate()
            
            self.spark.sparkContext.setLogLevel('WARN')
            
            self.logger.info('Spark session initialized successfully')
            return True
            
        except Exception as e:
            self.logger.error(f'Spark initialization failed: {e}')
            return False
```

### STEP 6: CREATE EXTERNAL TABLES IN PYSPARK
```python
    def create_external_tables_in_pyspark(self, uploaded_paths):
        """Create external tables pointing to uploaded Parquet files"""
        try:
            for table_name, file_path in uploaded_paths.items():
                # Read Parquet file
                df = self.spark.read.parquet(file_path)
                
                # Create temporary view for Hive data
                df.createOrReplaceTempView(f'hive_{table_name}')
                
                self.logger.info(f'Created external table for {table_name}')
            
            return True
            
        except Exception as e:
            self.logger.error(f'External table creation failed: {e}')
            return False
```

### STEP 7: EXECUTE PYSPARK EQUIVALENT CODE
```python
    def execute_pyspark_equivalent(self):
        """Execute equivalent PySpark transformations"""
        try:
            # Create mock sales table for testing
            self._create_mock_sales_table()
            
            # PySpark equivalent of Hive procedure
            
            # 1. Summary table logic (equivalent to dynamic query)
            summary_df = self.spark.sql(f'''
                SELECT product_id, SUM(sales) AS total_sales
                FROM sales_table
                WHERE sale_date BETWEEN '{self.test_start_date}' AND '{self.test_end_date}'
                GROUP BY product_id
            ''')
            
            summary_df.createOrReplaceTempView('pyspark_summary_table')
            
            # 2. Detailed sales summary (equivalent to cursor logic)
            detailed_df = self.spark.sql(f'''
                SELECT product_id, SUM(sales) AS total_sales
                FROM sales_table
                WHERE sale_date BETWEEN '{self.test_start_date}' AND '{self.test_end_date}'
                GROUP BY product_id
            ''')
            
            detailed_df.createOrReplaceTempView('pyspark_detailed_sales_summary')
            
            self.logger.info('PySpark equivalent executed successfully')
            return True
            
        except Exception as e:
            self.logger.error(f'PySpark execution failed: {e}')
            return False
    
    def _create_mock_sales_table(self):
        """Create mock sales data for testing"""
        from pyspark.sql.types import StructType, StructField, StringType, FloatType, DateType
        from datetime import timedelta
        
        schema = StructType([
            StructField('product_id', StringType(), True),
            StructField('sales', FloatType(), True),
            StructField('sale_date', DateType(), True)
        ])
        
        # Generate sample data
        sample_data = []
        for i in range(1000):
            sample_data.append((
                f'PROD_{i % 100}',
                float(100 + (i % 500)),
                datetime(2023, 1, 1) + timedelta(days=i % 365)
            ))
        
        sales_df = self.spark.createDataFrame(sample_data, schema)
        sales_df.createOrReplaceTempView('sales_table')
```

### STEP 8: COMPARE RESULTS
```python
    def compare_results(self):
        """Compare Hive and PySpark results"""
        comparison_results = {}
        
        try:
            for table_name in self.target_tables:
                self.logger.info(f'Comparing table: {table_name}')
                
                # Get DataFrames
                hive_df = self.spark.table(f'hive_{table_name}')
                pyspark_df = self.spark.table(f'pyspark_{table_name}')
                
                # Perform detailed comparison
                table_comparison = self._compare_dataframes(hive_df, pyspark_df, table_name)
                comparison_results[table_name] = table_comparison
            
            return comparison_results
            
        except Exception as e:
            self.logger.error(f'Comparison failed: {e}')
            return {}
    
    def _compare_dataframes(self, df1, df2, table_name):
        """Detailed DataFrame comparison"""
        comparison = {
            'table_name': table_name,
            'timestamp': datetime.now().isoformat(),
            'match_status': 'UNKNOWN',
            'row_count_match': False,
            'schema_match': False,
            'data_match': False,
            'differences': [],
            'statistics': {}
        }
        
        try:
            # Convert to Pandas for detailed comparison
            pdf1 = df1.toPandas()
            pdf2 = df2.toPandas()
            
            # Row count comparison
            count1, count2 = len(pdf1), len(pdf2)
            comparison['statistics']['hive_row_count'] = count1
            comparison['statistics']['pyspark_row_count'] = count2
            comparison['row_count_match'] = count1 == count2
            
            # Schema comparison
            schema1 = set(pdf1.columns)
            schema2 = set(pdf2.columns)
            comparison['schema_match'] = schema1 == schema2
            
            if not comparison['schema_match']:
                comparison['differences'].append({
                    'type': 'schema_mismatch',
                    'hive_columns': list(schema1),
                    'pyspark_columns': list(schema2)
                })
            
            # Data comparison
            if comparison['schema_match'] and comparison['row_count_match']:
                common_cols = list(schema1.intersection(schema2))
                if common_cols:
                    pdf1_sorted = pdf1[common_cols].sort_values(common_cols).reset_index(drop=True)
                    pdf2_sorted = pdf2[common_cols].sort_values(common_cols).reset_index(drop=True)
                    
                    comparison['data_match'] = pdf1_sorted.equals(pdf2_sorted)
                    
                    if not comparison['data_match']:
                        # Sample differences for troubleshooting
                        diff_mask = pdf1_sorted != pdf2_sorted
                        diff_rows = diff_mask.any(axis=1)
                        
                        if diff_rows.sum() > 0:
                            sample_diffs = []
                            for idx in diff_rows[diff_rows].index[:10]:  # First 10 differences
                                row_diff = {
                                    'row_index': idx,
                                    'hive_values': pdf1_sorted.iloc[idx].to_dict(),
                                    'pyspark_values': pdf2_sorted.iloc[idx].to_dict()
                                }
                                sample_diffs.append(row_diff)
                            
                            comparison['differences'].append({
                                'type': 'data_mismatch',
                                'total_diff_rows': diff_rows.sum(),
                                'sample_differences': sample_diffs
                            })
            
            # Overall match status
            if comparison['row_count_match'] and comparison['schema_match'] and comparison['data_match']:
                comparison['match_status'] = 'PASS'
            else:
                comparison['match_status'] = 'FAIL'
            
        except Exception as e:
            comparison['match_status'] = 'ERROR'
            comparison['error'] = str(e)
        
        return comparison
```

### STEP 9: GENERATE VALIDATION REPORT
```python
    def generate_validation_report(self, comparison_results):
        """Generate comprehensive validation report"""
        try:
            # Create report structure
            report = {
                'validation_summary': {
                    'execution_time': datetime.now().isoformat(),
                    'test_parameters': {
                        'start_date': self.test_start_date,
                        'end_date': self.test_end_date,
                        'target_tables': self.target_tables
                    },
                    'overall_status': 'PASS' if all(
                        result.get('match_status') == 'PASS'
                        for result in comparison_results.values()
                    ) else 'FAIL'
                },
                'table_comparisons': comparison_results,
                'recommendations': self._generate_recommendations(comparison_results)
            }
            
            # Save JSON report
            report_path = f'./validation_report_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json'
            with open(report_path, 'w') as f:
                json.dump(report, f, indent=2, default=str)
            
            # Generate HTML report
            self._generate_html_report(report)
            
            self.logger.info(f'Validation report generated: {report_path}')
            return report_path
            
        except Exception as e:
            self.logger.error(f'Report generation failed: {e}')
            return ''
    
    def _generate_recommendations(self, comparison_results):
        """Generate actionable recommendations"""
        recommendations = []
        
        for table_name, result in comparison_results.items():
            if result.get('match_status') == 'FAIL':
                if not result.get('row_count_match'):
                    recommendations.append(
                        f'Row count mismatch in {table_name}. Review data filtering logic.'
                    )
                
                if not result.get('schema_match'):
                    recommendations.append(
                        f'Schema mismatch in {table_name}. Verify column names and data types.'
                    )
                
                if not result.get('data_match'):
                    recommendations.append(
                        f'Data values mismatch in {table_name}. Check aggregation logic.'
                    )
        
        if not recommendations:
            recommendations.append('All validations passed successfully. Migration is ready for production.')
        
        return recommendations
```

### STEP 10: MAIN EXECUTION WORKFLOW
```python
    def run_full_validation(self):
        """Execute complete validation workflow"""
        try:
            self.logger.info('Starting Hive to PySpark migration validation...')
            
            # Step 1: Connect to Hive
            if not self.connect_to_hive():
                return False
            
            # Step 2: Execute Hive procedure
            if not self.execute_hive_procedure():
                return False
            
            # Step 3: Export to Parquet
            parquet_files = self.export_hive_tables_to_parquet()
            if not parquet_files:
                return False
            
            # Step 4: Upload to Databricks storage
            uploaded_paths = self.upload_to_databricks_storage(parquet_files)
            if not uploaded_paths:
                return False
            
            # Step 5: Initialize Spark
            if not self.initialize_spark_session():
                return False
            
            # Step 6: Create external tables
            if not self.create_external_tables_in_pyspark(uploaded_paths):
                return False
            
            # Step 7: Execute PySpark equivalent
            if not self.execute_pyspark_equivalent():
                return False
            
            # Step 8: Compare results
            comparison_results = self.compare_results()
            if not comparison_results:
                return False
            
            # Step 9: Generate report
            report_path = self.generate_validation_report(comparison_results)
            if not report_path:
                return False
            
            overall_status = all(
                result.get('match_status') == 'PASS'
                for result in comparison_results.values()
            )
            
            if overall_status:
                self.logger.info('All validations passed! Migration is successful.')
            else:
                self.logger.warning('Some validations failed. Review the report for details.')
            
            return True
            
        except Exception as e:
            self.logger.error(f'Validation workflow failed: {e}')
            return False
        
        finally:
            self.cleanup_resources()
    
    def cleanup_resources(self):
        """Clean up all resources"""
        try:
            if self.hive_conn:
                self.hive_conn.close()
            
            if self.spark:
                self.spark.stop()
            
            self.logger.info('Resources cleaned up successfully')
            
        except Exception as e:
            self.logger.error(f'Cleanup failed: {e}')
    
    def _setup_logging(self):
        """Setup comprehensive logging"""
        logger = logging.getLogger('HivePySparkRecon')
        logger.setLevel(logging.INFO)
        
        # File handler
        file_handler = logging.FileHandler(
            f'hive_pyspark_recon_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log'
        )
        
        # Console handler
        console_handler = logging.StreamHandler()
        
        # Formatter
        formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        file_handler.setFormatter(formatter)
        console_handler.setFormatter(formatter)
        
        logger.addHandler(file_handler)
        logger.addHandler(console_handler)
        
        return logger


# MAIN EXECUTION
def main():
    """Main execution function"""
    try:
        validator = HivePySparkReconValidator()
        success = validator.run_full_validation()
        
        if success:
            print('Validation completed successfully!')
            return 0
        else:
            print('Validation failed!')
            return 1
            
    except Exception as e:
        print(f'Critical error: {e}')
        return 1


if __name__ == '__main__':
    exit_code = main()
    sys.exit(exit_code)
```

## CONFIGURATION REQUIREMENTS

### Environment Variables
```bash
# Hive Connection
export HIVE_HOST=your_hive_host
export HIVE_PORT=10000
export HIVE_DATABASE=your_database
export HIVE_USERNAME=your_username
export HIVE_PASSWORD=your_password

# Databricks Connection
export DATABRICKS_HOST=your_databricks_host
export DATABRICKS_TOKEN=your_access_token

# Azure Storage
export AZURE_ACCOUNT=your_storage_account
export AZURE_KEY=your_storage_key
export AZURE_CONTAINER=migration-data
```

### Required Python Packages
```bash
pip install pyhive pandas pyarrow pyspark azure-storage-blob boto3 databricks-sql-connector
```

## VALIDATION REPORT STRUCTURE

The script generates comprehensive reports including:

1. **Execution Summary**
   - Overall validation status (PASS/FAIL)
   - Execution duration
   - Test parameters used

2. **Table-by-Table Comparison**
   - Row count validation
   - Schema comparison
   - Data value matching
   - Sample differences for troubleshooting

3. **Statistical Analysis**
   - Numeric column statistics comparison
   - Data distribution analysis
   - Performance metrics

4. **Actionable Recommendations**
   - Specific issues identified
   - Suggested remediation steps
   - Migration readiness assessment

## ERROR HANDLING AND RECOVERY

- **Connection Failures**: Automatic retry with exponential backoff
- **Data Transfer Issues**: Integrity checks and re-upload capabilities
- **Memory Management**: Batch processing for large datasets
- **Resource Cleanup**: Guaranteed cleanup in all scenarios

## PERFORMANCE OPTIMIZATIONS

- **Spark Configuration**: Adaptive query execution enabled
- **Data Formats**: Parquet for efficient storage and transfer
- **Parallel Processing**: Multi-threaded operations where applicable
- **Memory Optimization**: Efficient data type conversions

## SECURITY BEST PRACTICES

- **Credential Management**: Environment variables and secure vaults
- **Network Security**: Encrypted connections for all data transfers
- **Access Control**: Principle of least privilege
- **Audit Logging**: Comprehensive operation logging

This Recon Test Case provides production-ready validation for your Hive to PySpark migration, ensuring data integrity and transformation accuracy throughout the migration process.