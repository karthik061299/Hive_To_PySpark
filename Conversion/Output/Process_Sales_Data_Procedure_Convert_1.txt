=============================================
Author:        Ascendion AVA+
Created on:   
Description:   Hive stored procedure to PySpark conversion for sales data processing and aggregation
=============================================

#!/usr/bin/env python3
"""
Hive to PySpark Conversion
==========================

Original Hive Procedure: process_sales_data
Conversion Date: 2024
Description: Converts Hive stored procedure that processes sales data aggregation
             into PySpark DataFrame operations

Input Parameters:
- start_date (STRING): Start date for sales data filtering
- end_date (STRING): End date for sales data filtering

Original Hive Operations Converted:
1. Dynamic query execution for summary_table insertion
2. Temporary table creation with aggregated data
3. Cursor-based iteration and row-by-row insertion
4. Temporary table cleanup

PySpark Conversion Notes:
- Dynamic SQL replaced with DataFrame operations
- Cursor iteration replaced with DataFrame transformations
- Temporary tables replaced with DataFrame caching
- All operations use DataFrame API for better performance
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import sum as spark_sum, col
from pyspark.sql.types import StructType, StructField, StringType, FloatType
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def create_spark_session(app_name="ProcessSalesDataConversion"):
    spark = (
        SparkSession.builder
        .appName(app_name)
        # Enable Delta Lake support
        .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension")
        .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog")
        .config("spark.sql.adaptive.enabled", "true")
        .config("spark.sql.adaptive.coalescePartitions.enabled", "true")
        .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
        .getOrCreate()
    )
    spark.sparkContext.setLogLevel("WARN")
    return spark

def process_sales_data(spark, start_date, end_date):
    try:
        logger.info(f"Starting sales data processing for date range: {start_date} to {end_date}")
        
        # === Delta read instead of Hive table ===
        logger.info("Reading sales_table Delta data")
        sales_df = spark.read.format("delta").load("/delta/sales_table")
        
        filtered_sales_df = sales_df.filter(
            (col("sale_date") >= start_date) & 
            (col("sale_date") <= end_date)
        )
        
        # Summary aggregation
        logger.info("Aggregating sales data for summary_table")
        summary_aggregated_df = filtered_sales_df.groupBy("product_id") \
            .agg(spark_sum("sales").alias("total_sales"))
        
        # === Delta write ===
        logger.info("Writing aggregated data to Delta summary_table")
        summary_aggregated_df.write.format("delta") \
            .mode("append") \
            .save("/delta/summary_table")
        
        # Temporary aggregated dataset
        temp_sales_summary_df = filtered_sales_df.groupBy("product_id") \
            .agg(spark_sum("sales").alias("total_sales"))
        temp_sales_summary_df.cache()
        
        records_to_process = temp_sales_summary_df.collect()
        
        if records_to_process:
            detailed_schema = StructType([
                StructField("product_id", StringType(), True),
                StructField("total_sales", FloatType(), True)
            ])
            detailed_sales_df = spark.createDataFrame(records_to_process, detailed_schema)
            
            logger.info("Writing processed records to Delta detailed_sales_summary")
            detailed_sales_df.write.format("delta") \
                .mode("append") \
                .save("/delta/detailed_sales_summary")
        
        temp_sales_summary_df.unpersist()
        logger.info("Sales data processing completed successfully")
        return True
    
    except Exception as e:
        logger.error(f"Error processing sales data: {str(e)}")
        try:
            temp_sales_summary_df.unpersist()
        except:
            pass
        raise e

def main():
    spark = create_spark_session()
    try:
        start_date = "2024-01-01"
        end_date = "2024-12-31"
        success = process_sales_data(spark, start_date, end_date)
        if success:
            logger.info("Procedure executed successfully with Delta tables")
        else:
            logger.error("Procedure execution failed")
    finally:
        spark.stop()

if __name__ == "__main__":
    main()



"""
CONVERSION SUMMARY:
==================

1. DYNAMIC QUERY CONVERSION:
   - Hive: SET @dynamic_query = CONCAT(...); EXECUTE IMMEDIATE @dynamic_query;
   - PySpark: Direct DataFrame operations with groupBy and agg

2. TEMPORARY TABLE CONVERSION:
   - Hive: CREATE TEMPORARY TABLE temp_sales_summary AS SELECT ...
   - PySpark: DataFrame creation with .cache() for performance optimization

3. CURSOR ITERATION CONVERSION:
   - Hive: DECLARE cur CURSOR FOR ... + WHILE loop with FETCH
   - PySpark: DataFrame.collect() for small datasets or foreachPartition for large datasets

4. ROW-BY-ROW INSERTION CONVERSION:
   - Hive: INSERT INTO ... VALUES (product_id, total_sales) in loop
   - PySpark: Batch insertion using DataFrame.write.insertInto()

5. CLEANUP CONVERSION:
   - Hive: DROP TABLE temp_sales_summary
   - PySpark: DataFrame.unpersist() to release cached data

PERFORMANCE IMPROVEMENTS:
========================
- Eliminated row-by-row processing with batch operations
- Used DataFrame caching for repeated access patterns
- Leveraged Spark's adaptive query execution
- Replaced procedural logic with declarative DataFrame transformations

USAGE NOTES:
============
- Ensure tables (sales_table, summary_table, detailed_sales_summary) exist
- Adjust Spark configurations based on cluster resources
- For very large datasets, consider using foreachPartition instead of collect()
- Monitor memory usage when caching DataFrames
"""
