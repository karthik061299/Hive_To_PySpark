=============================================
Author:        Ascendion AVA+
Created on:   
Description:   Convert Hive stored procedure to PySpark for sales data aggregation and processing
=============================================

# -*- coding: utf-8 -*-
"""
Hive to PySpark Conversion: process_sales_data

Author: Ascendion AVA+
Created on: 
Description: Convert Hive stored procedure to PySpark for sales data aggregation and processing
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import sum as spark_sum, col
from pyspark.sql.types import StringType
import logging

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def process_sales_data(start_date: str, end_date: str) -> None:
    """
    Convert Hive stored procedure 'process_sales_data' to PySpark.
    
    This function performs the following operations:
    1. Aggregates sales data by product_id and sums sales amounts for date range
    2. Inserts aggregated data into summary_table
    3. Creates temporary aggregation for detailed processing
    4. Inserts each row into detailed_sales_summary table
    
    Args:
        start_date (str): Start date for filtering sales data (format: YYYY-MM-DD)
        end_date (str): End date for filtering sales data (format: YYYY-MM-DD)
    
    Returns:
        None
    """
    
    # Initialize Spark session with optimized configurations
    spark = SparkSession.builder \
        .appName("ProcessSalesData_HiveConversion") \
        .config("spark.sql.adaptive.enabled", "true") \
        .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
        .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \
        .enableHiveSupport() \
        .getOrCreate()
    
    try:
        logger.info(f"Starting sales data processing for date range: {start_date} to {end_date}")
        
        # Read source sales_table
        sales_df = spark.table("sales_table")
        
        # Validate input parameters
        if not start_date or not end_date:
            raise ValueError("start_date and end_date parameters are required")
        
        # Filter sales data by date range and perform aggregation
        # Equivalent to: SELECT product_id, SUM(sales) as total_sales 
        # FROM sales_table WHERE sale_date BETWEEN start_date AND end_date GROUP BY product_id
        filtered_sales_df = sales_df.filter(
            (col("sale_date") >= start_date) & (col("sale_date") <= end_date)
        )
        
        # Perform aggregation - equivalent to the dynamic SQL aggregation logic
        aggregated_sales_df = filtered_sales_df.groupBy("product_id") \
            .agg(spark_sum("sales").alias("total_sales")) \
            .select("product_id", "total_sales")
        
        # Cache the aggregated DataFrame for reuse (optimization for Spark)
        aggregated_sales_df.cache()
        
        logger.info("Aggregation completed, inserting into summary_table")
        
        # Operation 1: Insert aggregated data into summary_table
        # Equivalent to: INSERT INTO summary_table SELECT product_id, total_sales FROM ...
        aggregated_sales_df.write \
            .mode("append") \
            .insertInto("summary_table")
        
        logger.info("Data inserted into summary_table successfully")
        
        # Operation 2: Create temporary table equivalent (using DataFrame)
        # In PySpark, we use the cached DataFrame instead of creating a physical temp table
        temp_sales_summary_df = aggregated_sales_df
        
        logger.info("Processing detailed sales summary insertion")
        
        # Operation 3: Equivalent to cursor iteration - collect and insert row by row
        # Note: In production, consider using batch operations for better performance
        # This approach mimics the original cursor-based row-by-row processing
        temp_sales_rows = temp_sales_summary_df.collect()
        
        # Create DataFrame schema for detailed_sales_summary inserts
        for row in temp_sales_rows:
            # Create DataFrame for single row insertion
            single_row_df = spark.createDataFrame(
                [(row['product_id'], row['total_sales'])],
                ["product_id", "total_sales"]
            )
            
            # Insert single row into detailed_sales_summary table
            single_row_df.write \
                .mode("append") \
                .insertInto("detailed_sales_summary")
        
        logger.info(f"Inserted {len(temp_sales_rows)} rows into detailed_sales_summary")
        
        # Operation 4: Cleanup - unpersist cached DataFrame (equivalent to DROP temp table)
        aggregated_sales_df.unpersist()
        
        logger.info("Sales data processing completed successfully")
        
    except Exception as e:
        logger.error(f"Error processing sales data: {str(e)}")
        raise
    
    finally:
        # Clean up Spark session resources
        spark.catalog.clearCache()


def process_sales_data_optimized(start_date: str, end_date: str) -> None:
    """
    Optimized version of process_sales_data for better Spark performance.
    
    This version uses batch operations instead of row-by-row processing
    for better performance in distributed computing environment.
    
    Args:
        start_date (str): Start date for filtering sales data (format: YYYY-MM-DD)
        end_date (str): End date for filtering sales data (format: YYYY-MM-DD)
    
    Returns:
        None
    """
    
    # Initialize Spark session
    spark = SparkSession.builder \
        .appName("ProcessSalesData_Optimized") \
        .config("spark.sql.adaptive.enabled", "true") \
        .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
        .enableHiveSupport() \
        .getOrCreate()
    
    try:
        logger.info(f"Starting optimized sales data processing for: {start_date} to {end_date}")
        
        # Read and process sales data
        sales_df = spark.table("sales_table")
        
        # Single aggregation operation for both target tables
        aggregated_sales_df = sales_df.filter(
            (col("sale_date") >= start_date) & (col("sale_date") <= end_date)
        ).groupBy("product_id") \
         .agg(spark_sum("sales").alias("total_sales")) \
         .select("product_id", "total_sales")
        
        # Insert into both tables using batch operations
        aggregated_sales_df.write.mode("append").insertInto("summary_table")
        aggregated_sales_df.write.mode("append").insertInto("detailed_sales_summary")
        
        logger.info("Optimized sales data processing completed successfully")
        
    except Exception as e:
        logger.error(f"Error in optimized processing: {str(e)}")
        raise
    
    finally:
        spark.catalog.clearCache()


if __name__ == "__main__":
    # Example usage (commented out as per requirements - no sample data/testing)
    # process_sales_data("2024-01-01", "2024-01-31")
    # process_sales_data_optimized("2024-01-01", "2024-01-31")
    pass