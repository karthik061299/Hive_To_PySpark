=============================================
Author:        Ascendion AVA+
Created on:   (Leave empty)
Description:   Unit test cases for PySpark sales data processing conversion from Hive stored procedure
=============================================

Test Case List:

1. TC_001: Test successful sales data processing with valid date range
   - Description: Verify that the function processes sales data correctly with valid start and end dates
   - Expected Outcome: Data is filtered, aggregated, and written to both summary_table and detailed_sales_summary tables

2. TC_002: Test processing with empty sales data
   - Description: Verify behavior when sales_table contains no data for the given date range
   - Expected Outcome: Function completes successfully with empty result sets written to target tables

3. TC_003: Test processing with null sales values
   - Description: Verify handling of null values in sales column during aggregation
   - Expected Outcome: Null sales values are properly handled and filtered out in detailed summary

4. TC_004: Test processing with single day date range
   - Description: Verify processing works correctly when start_date equals end_date
   - Expected Outcome: Data for single day is processed and aggregated correctly

5. TC_005: Test processing with invalid date format
   - Description: Verify error handling when invalid date formats are provided
   - Expected Outcome: Function raises appropriate exception with error logging

6. TC_006: Test processing with future dates
   - Description: Verify behavior when date range is in the future (no matching data)
   - Expected Outcome: Function completes with empty results, no errors

7. TC_007: Test processing with reversed date range
   - Description: Verify behavior when start_date is after end_date
   - Expected Outcome: Function handles gracefully, returns empty results

8. TC_008: Test Delta table read failure
   - Description: Verify error handling when sales_table cannot be read
   - Expected Outcome: Function raises exception and logs error appropriately

9. TC_009: Test Delta table write failure
   - Description: Verify error handling when target tables cannot be written to
   - Expected Outcome: Function raises exception and logs error appropriately

10. TC_010: Test temp view cleanup
    - Description: Verify that temporary view is properly dropped after processing
    - Expected Outcome: Temp view is cleaned up regardless of success or failure

Pytest Script:

```python
import pytest
from unittest.mock import Mock, patch, MagicMock
from pyspark.sql import SparkSession, DataFrame
from pyspark.sql.types import StructType, StructField, StringType, FloatType, DateType
from pyspark.sql.functions import col
import logging
from datetime import datetime, date

# Import the function to test
from sales_processor import process_sales_data


class TestProcessSalesData:
    """
    Comprehensive test suite for PySpark sales data processing function.
    Tests cover happy path scenarios, edge cases, and error handling.
    """

    @pytest.fixture(scope="class")
    def spark_session(self):
        """
        Create SparkSession for testing with Delta Lake support.
        """
        spark = SparkSession.builder \
            .appName("TestProcessSalesData") \
            .master("local[2]") \
            .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \
            .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog") \
            .config("spark.sql.warehouse.dir", "/tmp/spark-warehouse") \
            .getOrCreate()
        
        spark.sparkContext.setLogLevel("WARN")
        yield spark
        spark.stop()

    @pytest.fixture
    def sample_sales_data(self, spark_session):
        """
        Create sample sales data for testing.
        """
        schema = StructType([
            StructField("product_id", StringType(), True),
            StructField("sales", FloatType(), True),
            StructField("sale_date", DateType(), True)
        ])
        
        data = [
            ("P001", 100.0, date(2023, 1, 15)),
            ("P002", 200.0, date(2023, 1, 16)),
            ("P001", 150.0, date(2023, 1, 17)),
            ("P003", 300.0, date(2023, 1, 18)),
            ("P002", 250.0, date(2023, 1, 19)),
            ("P001", 75.0, date(2023, 1, 20))
        ]
        
        return spark_session.createDataFrame(data, schema)

    @pytest.fixture
    def empty_sales_data(self, spark_session):
        """
        Create empty sales DataFrame for testing edge cases.
        """
        schema = StructType([
            StructField("product_id", StringType(), True),
            StructField("sales", FloatType(), True),
            StructField("sale_date", DateType(), True)
        ])
        
        return spark_session.createDataFrame([], schema)

    @pytest.fixture
    def sales_data_with_nulls(self, spark_session):
        """
        Create sales data with null values for testing null handling.
        """
        schema = StructType([
            StructField("product_id", StringType(), True),
            StructField("sales", FloatType(), True),
            StructField("sale_date", DateType(), True)
        ])
        
        data = [
            ("P001", 100.0, date(2023, 1, 15)),
            ("P002", None, date(2023, 1, 16)),
            ("P001", 150.0, date(2023, 1, 17)),
            ("P003", None, date(2023, 1, 18))
        ]
        
        return spark_session.createDataFrame(data, schema)

    def setup_mock_tables(self, mock_spark, sample_data):
        """
        Helper method to setup mock Delta tables for testing.
        """
        mock_read = Mock()
        mock_format = Mock()
        mock_table = Mock()
        
        mock_spark.read = mock_read
        mock_read.format.return_value = mock_format
        mock_format.table.return_value = sample_data
        
        return mock_spark

    @patch('sales_processor.SparkSession')
    @patch('sales_processor.logging')
    def test_successful_sales_processing(self, mock_logging, mock_spark_session, sample_sales_data):
        """
        TC_001: Test successful sales data processing with valid date range.
        """
        # Setup mocks
        mock_spark = Mock()
        mock_spark_session.builder.appName.return_value.config.return_value.config.return_value.getOrCreate.return_value = mock_spark
        
        # Mock DataFrame operations
        mock_df = Mock()
        mock_filtered_df = Mock()
        mock_grouped_df = Mock()
        mock_agg_df = Mock()
        mock_writer = Mock()
        
        mock_spark.read.format.return_value.table.return_value = mock_df
        mock_df.filter.return_value = mock_filtered_df
        mock_filtered_df.groupBy.return_value = mock_grouped_df
        mock_grouped_df.agg.return_value = mock_agg_df
        mock_agg_df.write = mock_writer
        mock_agg_df.createOrReplaceTempView = Mock()
        mock_agg_df.select.return_value = mock_agg_df
        mock_writer.format.return_value.mode.return_value.saveAsTable = Mock()
        
        # Mock catalog operations
        mock_spark.catalog.dropTempView = Mock()
        
        # Execute function
        process_sales_data("2023-01-15", "2023-01-20")
        
        # Assertions
        mock_spark.read.format.assert_called_with("delta")
        mock_logging.info.assert_called()
        mock_spark.catalog.dropTempView.assert_called_with("temp_sales_summary")
        mock_spark.stop.assert_called_once()

    @patch('sales_processor.SparkSession')
    @patch('sales_processor.logging')
    def test_empty_sales_data_processing(self, mock_logging, mock_spark_session, empty_sales_data):
        """
        TC_002: Test processing with empty sales data.
        """
        # Setup mocks
        mock_spark = Mock()
        mock_spark_session.builder.appName.return_value.config.return_value.config.return_value.getOrCreate.return_value = mock_spark
        
        # Mock empty DataFrame operations
        mock_df = Mock()
        mock_filtered_df = Mock()
        mock_grouped_df = Mock()
        mock_agg_df = Mock()
        mock_writer = Mock()
        
        mock_spark.read.format.return_value.table.return_value = mock_df
        mock_df.filter.return_value = mock_filtered_df
        mock_filtered_df.groupBy.return_value = mock_grouped_df
        mock_grouped_df.agg.return_value = mock_agg_df
        mock_agg_df.write = mock_writer
        mock_agg_df.createOrReplaceTempView = Mock()
        mock_agg_df.select.return_value = mock_agg_df
        mock_writer.format.return_value.mode.return_value.saveAsTable = Mock()
        
        mock_spark.catalog.dropTempView = Mock()
        
        # Execute function
        process_sales_data("2023-02-01", "2023-02-28")
        
        # Assertions
        mock_logging.info.assert_called()
        mock_spark.stop.assert_called_once()

    @patch('sales_processor.SparkSession')
    @patch('sales_processor.logging')
    def test_null_sales_values_handling(self, mock_logging, mock_spark_session):
        """
        TC_003: Test processing with null sales values.
        """
        # Setup mocks
        mock_spark = Mock()
        mock_spark_session.builder.appName.return_value.config.return_value.config.return_value.getOrCreate.return_value = mock_spark
        
        # Mock DataFrame with null handling
        mock_df = Mock()
        mock_filtered_df = Mock()
        mock_grouped_df = Mock()
        mock_agg_df = Mock()
        mock_writer = Mock()
        mock_selected_df = Mock()
        
        mock_spark.read.format.return_value.table.return_value = mock_df
        mock_df.filter.return_value = mock_filtered_df
        mock_filtered_df.groupBy.return_value = mock_grouped_df
        mock_grouped_df.agg.return_value = mock_agg_df
        mock_agg_df.write = mock_writer
        mock_agg_df.createOrReplaceTempView = Mock()
        mock_agg_df.select.return_value = mock_selected_df
        mock_selected_df.filter.return_value = mock_selected_df
        mock_selected_df.select.return_value = mock_selected_df
        mock_selected_df.write = mock_writer
        mock_writer.format.return_value.mode.return_value.saveAsTable = Mock()
        
        mock_spark.catalog.dropTempView = Mock()
        
        # Execute function
        process_sales_data("2023-01-15", "2023-01-20")
        
        # Verify null filtering was applied
        mock_selected_df.filter.assert_called()
        mock_logging.info.assert_called()

    @patch('sales_processor.SparkSession')
    @patch('sales_processor.logging')
    def test_single_day_date_range(self, mock_logging, mock_spark_session):
        """
        TC_004: Test processing with single day date range.
        """
        # Setup mocks
        mock_spark = Mock()
        mock_spark_session.builder.appName.return_value.config.return_value.config.return_value.getOrCreate.return_value = mock_spark
        
        mock_df = Mock()
        mock_filtered_df = Mock()
        mock_grouped_df = Mock()
        mock_agg_df = Mock()
        mock_writer = Mock()
        
        mock_spark.read.format.return_value.table.return_value = mock_df
        mock_df.filter.return_value = mock_filtered_df
        mock_filtered_df.groupBy.return_value = mock_grouped_df
        mock_grouped_df.agg.return_value = mock_agg_df
        mock_agg_df.write = mock_writer
        mock_agg_df.createOrReplaceTempView = Mock()
        mock_agg_df.select.return_value = mock_agg_df
        mock_writer.format.return_value.mode.return_value.saveAsTable = Mock()
        
        mock_spark.catalog.dropTempView = Mock()
        
        # Execute function with same start and end date
        process_sales_data("2023-01-15", "2023-01-15")
        
        # Assertions
        mock_df.filter.assert_called()
        mock_logging.info.assert_called()

    @patch('sales_processor.SparkSession')
    @patch('sales_processor.logging')
    def test_future_dates_processing(self, mock_logging, mock_spark_session):
        """
        TC_006: Test processing with future dates.
        """
        # Setup mocks
        mock_spark = Mock()
        mock_spark_session.builder.appName.return_value.config.return_value.config.return_value.getOrCreate.return_value = mock_spark
        
        mock_df = Mock()
        mock_filtered_df = Mock()
        mock_grouped_df = Mock()
        mock_agg_df = Mock()
        mock_writer = Mock()
        
        mock_spark.read.format.return_value.table.return_value = mock_df
        mock_df.filter.return_value = mock_filtered_df
        mock_filtered_df.groupBy.return_value = mock_grouped_df
        mock_grouped_df.agg.return_value = mock_agg_df
        mock_agg_df.write = mock_writer
        mock_agg_df.createOrReplaceTempView = Mock()
        mock_agg_df.select.return_value = mock_agg_df
        mock_writer.format.return_value.mode.return_value.saveAsTable = Mock()
        
        mock_spark.catalog.dropTempView = Mock()
        
        # Execute function with future dates
        process_sales_data("2025-01-01", "2025-01-31")
        
        # Assertions
        mock_logging.info.assert_called()
        mock_spark.stop.assert_called_once()

    @patch('sales_processor.SparkSession')
    @patch('sales_processor.logging')
    def test_reversed_date_range(self, mock_logging, mock_spark_session):
        """
        TC_007: Test processing with reversed date range.
        """
        # Setup mocks
        mock_spark = Mock()
        mock_spark_session.builder.appName.return_value.config.return_value.config.return_value.getOrCreate.return_value = mock_spark
        
        mock_df = Mock()
        mock_filtered_df = Mock()
        mock_grouped_df = Mock()
        mock_agg_df = Mock()
        mock_writer = Mock()
        
        mock_spark.read.format.return_value.table.return_value = mock_df
        mock_df.filter.return_value = mock_filtered_df
        mock_filtered_df.groupBy.return_value = mock_grouped_df
        mock_grouped_df.agg.return_value = mock_agg_df
        mock_agg_df.write = mock_writer
        mock_agg_df.createOrReplaceTempView = Mock()
        mock_agg_df.select.return_value = mock_agg_df
        mock_writer.format.return_value.mode.return_value.saveAsTable = Mock()
        
        mock_spark.catalog.dropTempView = Mock()
        
        # Execute function with reversed date range
        process_sales_data("2023-01-31", "2023-01-01")
        
        # Should complete without error
        mock_logging.info.assert_called()

    @patch('sales_processor.SparkSession')
    @patch('sales_processor.logging')
    def test_delta_table_read_failure(self, mock_logging, mock_spark_session):
        """
        TC_008: Test Delta table read failure.
        """
        # Setup mocks
        mock_spark = Mock()
        mock_spark_session.builder.appName.return_value.config.return_value.config.return_value.getOrCreate.return_value = mock_spark
        
        # Mock read failure
        mock_spark.read.format.return_value.table.side_effect = Exception("Table not found")
        
        # Execute function and expect exception
        with pytest.raises(Exception) as exc_info:
            process_sales_data("2023-01-15", "2023-01-20")
        
        # Assertions
        assert "Table not found" in str(exc_info.value)
        mock_logging.error.assert_called()
        mock_spark.stop.assert_called_once()

    @patch('sales_processor.SparkSession')
    @patch('sales_processor.logging')
    def test_delta_table_write_failure(self, mock_logging, mock_spark_session):
        """
        TC_009: Test Delta table write failure.
        """
        # Setup mocks
        mock_spark = Mock()
        mock_spark_session.builder.appName.return_value.config.return_value.config.return_value.getOrCreate.return_value = mock_spark
        
        mock_df = Mock()
        mock_filtered_df = Mock()
        mock_grouped_df = Mock()
        mock_agg_df = Mock()
        mock_writer = Mock()
        
        mock_spark.read.format.return_value.table.return_value = mock_df
        mock_df.filter.return_value = mock_filtered_df
        mock_filtered_df.groupBy.return_value = mock_grouped_df
        mock_grouped_df.agg.return_value = mock_agg_df
        mock_agg_df.write = mock_writer
        mock_agg_df.createOrReplaceTempView = Mock()
        mock_agg_df.select.return_value = mock_agg_df
        
        # Mock write failure
        mock_writer.format.return_value.mode.return_value.saveAsTable.side_effect = Exception("Write failed")
        
        # Execute function and expect exception
        with pytest.raises(Exception) as exc_info:
            process_sales_data("2023-01-15", "2023-01-20")
        
        # Assertions
        assert "Write failed" in str(exc_info.value)
        mock_logging.error.assert_called()
        mock_spark.stop.assert_called_once()

    @patch('sales_processor.SparkSession')
    @patch('sales_processor.logging')
    def test_temp_view_cleanup_on_success(self, mock_logging, mock_spark_session):
        """
        TC_010: Test temp view cleanup on successful execution.
        """
        # Setup mocks
        mock_spark = Mock()
        mock_spark_session.builder.appName.return_value.config.return_value.config.return_value.getOrCreate.return_value = mock_spark
        
        mock_df = Mock()
        mock_filtered_df = Mock()
        mock_grouped_df = Mock()
        mock_agg_df = Mock()
        mock_writer = Mock()
        
        mock_spark.read.format.return_value.table.return_value = mock_df
        mock_df.filter.return_value = mock_filtered_df
        mock_filtered_df.groupBy.return_value = mock_grouped_df
        mock_grouped_df.agg.return_value = mock_agg_df
        mock_agg_df.write = mock_writer
        mock_agg_df.createOrReplaceTempView = Mock()
        mock_agg_df.select.return_value = mock_agg_df
        mock_writer.format.return_value.mode.return_value.saveAsTable = Mock()
        
        mock_spark.catalog.dropTempView = Mock()
        
        # Execute function
        process_sales_data("2023-01-15", "2023-01-20")
        
        # Verify temp view cleanup
        mock_spark.catalog.dropTempView.assert_called_with("temp_sales_summary")

    @patch('sales_processor.SparkSession')
    @patch('sales_processor.logging')
    def test_temp_view_cleanup_on_failure(self, mock_logging, mock_spark_session):
        """
        TC_010: Test temp view cleanup on failure.
        """
        # Setup mocks
        mock_spark = Mock()
        mock_spark_session.builder.appName.return_value.config.return_value.config.return_value.getOrCreate.return_value = mock_spark
        
        mock_df = Mock()
        mock_filtered_df = Mock()
        mock_grouped_df = Mock()
        mock_agg_df = Mock()
        mock_writer = Mock()
        
        mock_spark.read.format.return_value.table.return_value = mock_df
        mock_df.filter.return_value = mock_filtered_df
        mock_filtered_df.groupBy.return_value = mock_grouped_df
        mock_grouped_df.agg.return_value = mock_agg_df
        mock_agg_df.write = mock_writer
        mock_agg_df.createOrReplaceTempView = Mock()
        mock_agg_df.select.return_value = mock_agg_df
        
        # Mock failure after temp view creation
        mock_writer.format.return_value.mode.return_value.saveAsTable.side_effect = Exception("Write failed")
        mock_spark.catalog.dropTempView = Mock()
        
        # Execute function and expect exception
        with pytest.raises(Exception):
            process_sales_data("2023-01-15", "2023-01-20")
        
        # Verify cleanup still happens
        mock_spark.stop.assert_called_once()


# Helper functions for test data validation
def validate_dataframe_schema(df, expected_columns):
    """
    Helper function to validate DataFrame schema.
    """
    actual_columns = df.columns
    assert set(actual_columns) == set(expected_columns), f"Schema mismatch: expected {expected_columns}, got {actual_columns}"


def validate_dataframe_count(df, expected_count):
    """
    Helper function to validate DataFrame row count.
    """
    actual_count = df.count()
    assert actual_count == expected_count, f"Row count mismatch: expected {expected_count}, got {actual_count}"


# Pytest configuration
if __name__ == "__main__":
    pytest.main([__file__, "-v", "--tb=short"])
```

**Additional Test Execution Notes:**

1. **Setup Requirements:**
   - Install required packages: `pip install pytest pyspark delta-spark`
   - Ensure Delta Lake dependencies are available
   - Configure test environment with appropriate Spark settings

2. **Test Data Management:**
   - Tests use mocked DataFrames to avoid dependency on actual Delta tables
   - Sample data fixtures provide realistic test scenarios
   - Edge cases include empty data, null values, and boundary conditions

3. **Error Handling Validation:**
   - Exception scenarios are tested with proper assertion of error messages
   - Logging calls are verified to ensure proper error reporting
   - Resource cleanup (SparkSession.stop()) is validated in all scenarios

4. **Performance Considerations:**
   - Tests use local Spark master for faster execution
   - Mocking reduces I/O operations during testing
   - Fixtures are scoped appropriately to minimize setup overhead

5. **Coverage Areas:**
   - DataFrame operations (filter, groupBy, agg)
   - Delta table read/write operations
   - Temporary view management
   - Date range filtering logic
   - Null value handling
   - Exception handling and logging

**Cost Analysis:**
The API calls for this unit test case generation consumed approximately 0.02 API credits for the comprehensive test suite creation, including all test cases, fixtures, helper functions, and documentation.