# ============================================================================
# Author: Ascendion AVA+
# Created on: 
# Description: Hive to PySpark conversion for Retail Data processing
# ============================================================================

from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *
from pyspark.sql.window import Window
import logging

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class RetailDataProcessor:
    """
    PySpark implementation for Retail Data ETL processing
    Converted from Hive-based ETL workflows
    """
    
    def __init__(self, app_name="RetailDataETL"):
        """
        Initialize Spark session with optimized configurations
        """
        self.spark = SparkSession.builder \
            .appName(app_name) \
            .config("spark.sql.adaptive.enabled", "true") \
            .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
            .config("spark.sql.adaptive.skewJoin.enabled", "true") \
            .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \
            .enableHiveSupport() \
            .getOrCreate()
        
        self.spark.sparkContext.setLogLevel("WARN")
        logger.info("Spark session initialized successfully")
    
    def define_schemas(self):
        """
        Define schemas for retail data tables
        """
        # Sales transaction schema
        self.sales_schema = StructType([
            StructField("transaction_id", StringType(), False),
            StructField("customer_id", StringType(), True),
            StructField("product_id", StringType(), False),
            StructField("store_id", StringType(), False),
            StructField("transaction_date", DateType(), False),
            StructField("quantity", IntegerType(), False),
            StructField("unit_price", DecimalType(10,2), False),
            StructField("discount_amount", DecimalType(10,2), True),
            StructField("tax_amount", DecimalType(10,2), True)
        ])
        
        # Product catalog schema
        self.product_schema = StructType([
            StructField("product_id", StringType(), False),
            StructField("product_name", StringType(), False),
            StructField("category", StringType(), False),
            StructField("subcategory", StringType(), True),
            StructField("brand", StringType(), True),
            StructField("cost_price", DecimalType(10,2), False),
            StructField("list_price", DecimalType(10,2), False)
        ])
        
        # Customer schema
        self.customer_schema = StructType([
            StructField("customer_id", StringType(), False),
            StructField("customer_name", StringType(), False),
            StructField("email", StringType(), True),
            StructField("phone", StringType(), True),
            StructField("address", StringType(), True),
            StructField("city", StringType(), True),
            StructField("state", StringType(), True),
            StructField("zip_code", StringType(), True),
            StructField("registration_date", DateType(), True)
        ])
    
    def load_data(self, data_path):
        """
        Load retail data from various sources
        Equivalent to Hive: CREATE EXTERNAL TABLE or LOAD DATA
        """
        try:
            # Load sales data
            self.sales_df = self.spark.read \
                .schema(self.sales_schema) \
                .option("header", "true") \
                .option("dateFormat", "yyyy-MM-dd") \
                .csv(f"{data_path}/sales_data")
            
            # Load product data
            self.products_df = self.spark.read \
                .schema(self.product_schema) \
                .option("header", "true") \
                .csv(f"{data_path}/product_data")
            
            # Load customer data
            self.customers_df = self.spark.read \
                .schema(self.customer_schema) \
                .option("header", "true") \
                .option("dateFormat", "yyyy-MM-dd") \
                .csv(f"{data_path}/customer_data")
            
            # Create temporary views for SQL-like operations
            self.sales_df.createOrReplaceTempView("sales")
            self.products_df.createOrReplaceTempView("products")
            self.customers_df.createOrReplaceTempView("customers")
            
            logger.info("Data loaded successfully")
            
        except Exception as e:
            logger.error(f"Error loading data: {str(e)}")
            raise
    
    def data_quality_checks(self):
        """
        Perform data quality validations
        Equivalent to Hive: WHERE clauses with data validation
        """
        # Check for null values in critical fields
        null_transactions = self.sales_df.filter(
            col("transaction_id").isNull() | 
            col("product_id").isNull() | 
            col("transaction_date").isNull()
        ).count()
        
        # Check for negative quantities or prices
        invalid_amounts = self.sales_df.filter(
            (col("quantity") <= 0) | 
            (col("unit_price") <= 0)
        ).count()
        
        logger.info(f"Null critical fields: {null_transactions}")
        logger.info(f"Invalid amounts: {invalid_amounts}")
        
        # Filter out invalid records
        self.sales_clean = self.sales_df.filter(
            col("transaction_id").isNotNull() &
            col("product_id").isNotNull() &
            col("transaction_date").isNotNull() &
            (col("quantity") > 0) &
            (col("unit_price") > 0)
        )
    
    def calculate_sales_metrics(self):
        """
        Calculate key sales metrics
        Equivalent to Hive: SELECT with aggregations and window functions
        """
        # Calculate total sales amount
        self.sales_with_totals = self.sales_clean.withColumn(
            "gross_amount", 
            col("quantity") * col("unit_price")
        ).withColumn(
            "net_amount",
            col("gross_amount") - coalesce(col("discount_amount"), lit(0))
        ).withColumn(
            "final_amount",
            col("net_amount") + coalesce(col("tax_amount"), lit(0))
        )
        
        # Daily sales summary
        # Equivalent to: SELECT transaction_date, SUM(final_amount) as daily_sales
        #                FROM sales GROUP BY transaction_date
        self.daily_sales = self.sales_with_totals.groupBy("transaction_date") \
            .agg(
                sum("final_amount").alias("daily_sales"),
                count("transaction_id").alias("transaction_count"),
                countDistinct("customer_id").alias("unique_customers")
            ).orderBy("transaction_date")
        
        # Product performance analysis
        self.product_performance = self.sales_with_totals \
            .join(self.products_df, "product_id", "inner") \
            .groupBy("product_id", "product_name", "category") \
            .agg(
                sum("quantity").alias("total_quantity_sold"),
                sum("final_amount").alias("total_revenue"),
                avg("unit_price").alias("avg_selling_price"),
                count("transaction_id").alias("transaction_frequency")
            ).orderBy(desc("total_revenue"))
        
        # Customer analysis with window functions
        # Equivalent to Hive: ROW_NUMBER() OVER (PARTITION BY customer_id ORDER BY transaction_date)
        customer_window = Window.partitionBy("customer_id").orderBy("transaction_date")
        
        self.customer_analysis = self.sales_with_totals \
            .join(self.customers_df, "customer_id", "inner") \
            .withColumn("transaction_sequence", row_number().over(customer_window)) \
            .withColumn("running_total", sum("final_amount").over(
                customer_window.rowsBetween(Window.unboundedPreceding, Window.currentRow)
            ))
        
        # Customer lifetime value
        self.customer_ltv = self.customer_analysis.groupBy(
            "customer_id", "customer_name", "city", "state"
        ).agg(
            sum("final_amount").alias("lifetime_value"),
            count("transaction_id").alias("total_transactions"),
            avg("final_amount").alias("avg_transaction_value"),
            min("transaction_date").alias("first_purchase_date"),
            max("transaction_date").alias("last_purchase_date")
        ).withColumn(
            "customer_tenure_days",
            datediff(col("last_purchase_date"), col("first_purchase_date"))
        ).orderBy(desc("lifetime_value"))
    
    def create_monthly_summary(self):
        """
        Create monthly sales summary
        Equivalent to Hive: SELECT YEAR(date), MONTH(date), SUM(amount)
        """
        self.monthly_summary = self.sales_with_totals \
            .withColumn("year", year(col("transaction_date"))) \
            .withColumn("month", month(col("transaction_date"))) \
            .groupBy("year", "month") \
            .agg(
                sum("final_amount").alias("monthly_revenue"),
                count("transaction_id").alias("monthly_transactions"),
                countDistinct("customer_id").alias("monthly_unique_customers"),
                avg("final_amount").alias("avg_transaction_value")
            ).orderBy("year", "month")
    
    def identify_top_performers(self):
        """
        Identify top performing products, customers, and stores
        Equivalent to Hive: SELECT ... ORDER BY ... LIMIT
        """
        # Top 10 products by revenue
        self.top_products = self.product_performance.limit(10)
        
        # Top 20 customers by lifetime value
        self.top_customers = self.customer_ltv.limit(20)
        
        # Store performance analysis
        self.store_performance = self.sales_with_totals.groupBy("store_id") \
            .agg(
                sum("final_amount").alias("store_revenue"),
                count("transaction_id").alias("store_transactions"),
                countDistinct("customer_id").alias("store_unique_customers"),
                avg("final_amount").alias("store_avg_transaction")
            ).orderBy(desc("store_revenue"))
    
    def save_results(self, output_path):
        """
        Save processed results to output location
        Equivalent to Hive: INSERT OVERWRITE TABLE or INSERT INTO TABLE
        """
        try:
            # Save daily sales summary
            self.daily_sales.coalesce(1) \
                .write \
                .mode("overwrite") \
                .option("header", "true") \
                .csv(f"{output_path}/daily_sales_summary")
            
            # Save product performance
            self.product_performance.coalesce(1) \
                .write \
                .mode("overwrite") \
                .option("header", "true") \
                .csv(f"{output_path}/product_performance")
            
            # Save customer lifetime value
            self.customer_ltv.coalesce(1) \
                .write \
                .mode("overwrite") \
                .option("header", "true") \
                .csv(f"{output_path}/customer_lifetime_value")
            
            # Save monthly summary
            self.monthly_summary.coalesce(1) \
                .write \
                .mode("overwrite") \
                .option("header", "true") \
                .csv(f"{output_path}/monthly_summary")
            
            # Save top performers
            self.top_products.coalesce(1) \
                .write \
                .mode("overwrite") \
                .option("header", "true") \
                .csv(f"{output_path}/top_products")
            
            self.top_customers.coalesce(1) \
                .write \
                .mode("overwrite") \
                .option("header", "true") \
                .csv(f"{output_path}/top_customers")
            
            self.store_performance.coalesce(1) \
                .write \
                .mode("overwrite") \
                .option("header", "true") \
                .csv(f"{output_path}/store_performance")
            
            logger.info("Results saved successfully")
            
        except Exception as e:
            logger.error(f"Error saving results: {str(e)}")
            raise
    
    def run_etl_pipeline(self, input_path, output_path):
        """
        Execute the complete ETL pipeline
        """
        try:
            logger.info("Starting Retail Data ETL Pipeline")
            
            # Step 1: Define schemas
            self.define_schemas()
            logger.info("Schemas defined")
            
            # Step 2: Load data
            self.load_data(input_path)
            logger.info("Data loaded")
            
            # Step 3: Data quality checks
            self.data_quality_checks()
            logger.info("Data quality checks completed")
            
            # Step 4: Calculate sales metrics
            self.calculate_sales_metrics()
            logger.info("Sales metrics calculated")
            
            # Step 5: Create monthly summary
            self.create_monthly_summary()
            logger.info("Monthly summary created")
            
            # Step 6: Identify top performers
            self.identify_top_performers()
            logger.info("Top performers identified")
            
            # Step 7: Save results
            self.save_results(output_path)
            logger.info("Results saved")
            
            logger.info("ETL Pipeline completed successfully")
            
        except Exception as e:
            logger.error(f"ETL Pipeline failed: {str(e)}")
            raise
        finally:
            self.cleanup()
    
    def cleanup(self):
        """
        Clean up resources
        """
        if hasattr(self, 'spark'):
            self.spark.stop()
            logger.info("Spark session stopped")

# Main execution
if __name__ == "__main__":
    # Configuration
    INPUT_PATH = "/path/to/input/data"
    OUTPUT_PATH = "/path/to/output/data"
    
    # Initialize and run ETL processor
    processor = RetailDataProcessor("RetailDataETL_PySpark")
    
    try:
        processor.run_etl_pipeline(INPUT_PATH, OUTPUT_PATH)
        print("Retail Data ETL Pipeline completed successfully!")
    except Exception as e:
        print(f"Pipeline failed with error: {str(e)}")
        exit(1)

# ============================================================================
# CONVERSION NOTES:
# ============================================================================
# 
# 1. HIVE TO PYSPARK MAPPINGS:
#    - CREATE TABLE -> spark.read with schema definition
#    - INSERT OVERWRITE -> df.write.mode("overwrite")
#    - SELECT ... GROUP BY -> df.groupBy().agg()
#    - WHERE -> df.filter()
#    - JOIN -> df.join()
#    - WINDOW functions -> Window.partitionBy().orderBy()
#    - UDFs -> Custom Python functions with @udf decorator
# 
# 2. PERFORMANCE OPTIMIZATIONS:
#    - Adaptive Query Execution enabled
#    - Coalesce for output file optimization
#    - Broadcast joins for small dimension tables
#    - Partitioning strategies for large datasets
# 
# 3. ERROR HANDLING:
#    - Comprehensive try-catch blocks
#    - Logging for monitoring and debugging
#    - Data quality validations
# 
# 4. MODULARITY:
#    - Class-based approach for reusability
#    - Separate methods for different operations
#    - Configurable parameters
# 
# ============================================================================