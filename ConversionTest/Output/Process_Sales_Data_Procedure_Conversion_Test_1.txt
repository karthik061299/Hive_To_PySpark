# HIVE TO PYSPARK CONVERSION TEST CASE

## METADATA REQUIREMENTS
=============================================
Author:        Ascendion AVA+
Created on:   
Description:   Conversion test case for Hive stored procedure to PySpark transformation for sales data processing
=============================================

## SYNTAX CHANGE DETECTION

### 1. SQL Function Conversions

#### COLLECT_SET() Function
**Hive:**
```sql
SELECT customer_id, COLLECT_SET(product_id) as products
FROM sales_data
GROUP BY customer_id;
```

**PySpark:**
```python
from pyspark.sql.functions import collect_set

result = sales_data.groupBy("customer_id").agg(collect_set("product_id").alias("products"))
```

#### CONCAT_WS() Function
**Hive:**
```sql
SELECT CONCAT_WS(',', first_name, last_name) as full_name
FROM customers;
```

**PySpark:**
```python
from pyspark.sql.functions import concat_ws

result = customers.select(concat_ws(",", "first_name", "last_name").alias("full_name"))
```

### 2. Data Type Transformations

**Hive DDL:**
```sql
CREATE TABLE sales_summary (
    customer_id STRING,
    total_amount DECIMAL(10,2),
    is_active BOOLEAN,
    created_date DATE
);
```

**PySpark Schema:**
```python
from pyspark.sql.types import StructType, StructField, StringType, DecimalType, BooleanType, DateType

schema = StructType([
    StructField("customer_id", StringType(), True),
    StructField("total_amount", DecimalType(10,2), True),
    StructField("is_active", BooleanType(), True),
    StructField("created_date", DateType(), True)
])
```

### 3. Query Structure Modifications

#### JOIN Strategies
**Hive:**
```sql
SELECT /*+ MAPJOIN(c) */ s.*, c.customer_name
FROM sales s
JOIN customers c ON s.customer_id = c.customer_id;
```

**PySpark:**
```python
from pyspark.sql.functions import broadcast

result = sales.join(broadcast(customers), "customer_id", "inner")
```

### 4. Aggregation and Window Function Changes

**Hive:**
```sql
SELECT customer_id, 
       SUM(amount) OVER (PARTITION BY customer_id ORDER BY sale_date) as running_total
FROM sales;
```

**PySpark:**
```python
from pyspark.sql.window import Window
from pyspark.sql.functions import sum as spark_sum

window_spec = Window.partitionBy("customer_id").orderBy("sale_date")
result = sales.select("*", spark_sum("amount").over(window_spec).alias("running_total"))
```

### 5. NULL Value Handling

**Hive:**
```sql
SELECT COALESCE(customer_name, 'Unknown') as customer_name
FROM customers;
```

**PySpark:**
```python
from pyspark.sql.functions import coalesce, lit

result = customers.select(coalesce("customer_name", lit("Unknown")).alias("customer_name"))
```

## RECOMMENDED MANUAL INTERVENTIONS

### 1. Performance Optimizations

#### Broadcast Joins
- **Issue**: Hive MAPJOIN hints need conversion to PySpark broadcast
- **Solution**: Use `broadcast()` function for small dimension tables
- **Code Example**:
```python
from pyspark.sql.functions import broadcast
result = large_table.join(broadcast(small_table), "key")
```

#### Repartitioning
- **Issue**: Hive bucketing needs conversion to PySpark partitioning
- **Solution**: Use `repartition()` or `coalesce()` for optimal parallelism
- **Code Example**:
```python
df_optimized = df.repartition("partition_column")
```

### 2. Edge Case Handling

#### Data Type Inconsistencies
- **Issue**: Implicit type conversions in Hive may fail in PySpark
- **Solution**: Explicit casting using `cast()` function
- **Code Example**:
```python
from pyspark.sql.functions import col
df = df.withColumn("amount", col("amount").cast("decimal(10,2)"))
```

### 3. Complex Expressions Requiring UDFs

#### Custom Business Logic
- **Issue**: Complex Hive UDFs need conversion to PySpark UDFs
- **Solution**: Create Python UDFs or use built-in functions
- **Code Example**:
```python
from pyspark.sql.functions import udf
from pyspark.sql.types import StringType

@udf(returnType=StringType())
def custom_logic(value):
    # Custom business logic here
    return processed_value

df = df.withColumn("processed_column", custom_logic("input_column"))
```

## COMPREHENSIVE TEST CASE LIST

### Syntax Changes Test Cases

#### TC001: COLLECT_SET Function Conversion
- **Test Case ID**: TC001
- **Test Case Description**: Verify COLLECT_SET() function conversion from Hive to PySpark
- **Expected Outcome**: PySpark groupBy().agg(collect_set()) produces same results as Hive COLLECT_SET()

#### TC002: Data Type Schema Conversion
- **Test Case ID**: TC002
- **Test Case Description**: Validate data type mappings between Hive DDL and PySpark StructType
- **Expected Outcome**: Schema compatibility and data integrity maintained

#### TC003: JOIN Strategy Conversion
- **Test Case ID**: TC003
- **Test Case Description**: Test MAPJOIN hint conversion to broadcast join
- **Expected Outcome**: Performance improvement with correct join results

#### TC004: Window Function Conversion
- **Test Case ID**: TC004
- **Test Case Description**: Verify window function syntax conversion
- **Expected Outcome**: Identical results for running totals and rankings

#### TC005: NULL Handling Conversion
- **Test Case ID**: TC005
- **Test Case Description**: Test NULL value handling functions conversion
- **Expected Outcome**: Consistent NULL handling behavior

### Manual Interventions Test Cases

#### TC006: Broadcast Join Optimization
- **Test Case ID**: TC006
- **Test Case Description**: Validate broadcast join performance optimization
- **Expected Outcome**: Improved query performance with correct results

#### TC007: Repartitioning Strategy
- **Test Case ID**: TC007
- **Test Case Description**: Test data repartitioning for optimal performance
- **Expected Outcome**: Balanced data distribution across partitions

#### TC008: UDF Conversion
- **Test Case ID**: TC008
- **Test Case Description**: Verify custom UDF conversion from Hive to PySpark
- **Expected Outcome**: Functional equivalence with maintained performance

#### TC009: Error Handling
- **Test Case ID**: TC009
- **Test Case Description**: Test error handling and exception management
- **Expected Outcome**: Graceful error handling with appropriate logging

#### TC010: Memory Optimization
- **Test Case ID**: TC010
- **Test Case Description**: Validate memory usage optimization techniques
- **Expected Outcome**: Reduced memory footprint with maintained functionality

## PYTEST SCRIPTS

### Test Script for Syntax Changes

```python
import pytest
from pyspark.sql import SparkSession
from pyspark.sql.functions import collect_set, concat_ws, coalesce, lit, broadcast
from pyspark.sql.types import StructType, StructField, StringType, DecimalType, BooleanType, DateType
from pyspark.sql.window import Window
from pyspark.sql.functions import sum as spark_sum

class TestHiveToPySparkConversion:
    
    @pytest.fixture(scope="class")
    def spark_session(self):
        spark = SparkSession.builder \
            .appName("HiveToPySparkConversionTest") \
            .config("spark.sql.adaptive.enabled", "true") \
            .getOrCreate()
        yield spark
        spark.stop()
    
    @pytest.fixture
    def sample_sales_data(self, spark_session):
        data = [
            ("C001", "P001", 100.50, "2023-01-01"),
            ("C001", "P002", 200.75, "2023-01-02"),
            ("C002", "P001", 150.25, "2023-01-01"),
            ("C002", "P003", 300.00, "2023-01-03")
        ]
        schema = ["customer_id", "product_id", "amount", "sale_date"]
        return spark_session.createDataFrame(data, schema)
    
    @pytest.fixture
    def sample_customers_data(self, spark_session):
        data = [
            ("C001", "John", "Doe", True),
            ("C002", "Jane", "Smith", False),
            ("C003", None, "Johnson", True)
        ]
        schema = ["customer_id", "first_name", "last_name", "is_active"]
        return spark_session.createDataFrame(data, schema)
    
    def test_collect_set_conversion(self, spark_session, sample_sales_data):
        """TC001: Test COLLECT_SET function conversion"""
        result = sample_sales_data.groupBy("customer_id").agg(
            collect_set("product_id").alias("products")
        )
        
        collected_result = result.collect()
        assert len(collected_result) == 2
        
        # Verify customer C001 has products P001 and P002
        c001_products = [row.products for row in collected_result if row.customer_id == "C001"][0]
        assert set(c001_products) == {"P001", "P002"}
    
    def test_data_type_schema_conversion(self, spark_session):
        """TC002: Test data type schema conversion"""
        schema = StructType([
            StructField("customer_id", StringType(), True),
            StructField("total_amount", DecimalType(10,2), True),
            StructField("is_active", BooleanType(), True)
        ])
        
        data = [("C001", 100.50, True), ("C002", 200.75, False)]
        df = spark_session.createDataFrame(data, schema)
        
        assert df.schema["customer_id"].dataType == StringType()
        assert df.schema["total_amount"].dataType == DecimalType(10,2)
        assert df.schema["is_active"].dataType == BooleanType()
    
    def test_join_strategy_conversion(self, spark_session, sample_sales_data, sample_customers_data):
        """TC003: Test JOIN strategy conversion with broadcast"""
        # Test broadcast join
        result = sample_sales_data.join(
            broadcast(sample_customers_data), 
            "customer_id", 
            "inner"
        )
        
        collected_result = result.collect()
        assert len(collected_result) == 4  # All sales records should join with customers
        
        # Verify join correctness
        customer_ids = [row.customer_id for row in collected_result]
        assert "C001" in customer_ids
        assert "C002" in customer_ids
    
    def test_window_function_conversion(self, spark_session, sample_sales_data):
        """TC004: Test window function conversion"""
        window_spec = Window.partitionBy("customer_id").orderBy("sale_date")
        result = sample_sales_data.select(
            "*", 
            spark_sum("amount").over(window_spec).alias("running_total")
        )
        
        collected_result = result.collect()
        
        # Verify running total calculation
        c001_records = [row for row in collected_result if row.customer_id == "C001"]
        c001_records.sort(key=lambda x: x.sale_date)
        
        assert float(c001_records[0].running_total) == 100.50
        assert float(c001_records[1].running_total) == 301.25  # 100.50 + 200.75
    
    def test_null_handling_conversion(self, spark_session, sample_customers_data):
        """TC005: Test NULL handling conversion"""
        result = sample_customers_data.select(
            "customer_id",
            coalesce("first_name", lit("Unknown")).alias("first_name_clean")
        )
        
        collected_result = result.collect()
        
        # Find the record with NULL first_name (C003)
        c003_record = [row for row in collected_result if row.customer_id == "C003"][0]
        assert c003_record.first_name_clean == "Unknown"
    
    def test_concat_ws_conversion(self, spark_session, sample_customers_data):
        """Test CONCAT_WS function conversion"""
        result = sample_customers_data.select(
            "customer_id",
            concat_ws(",", "first_name", "last_name").alias("full_name")
        )
        
        collected_result = result.collect()
        
        # Verify concatenation
        c001_record = [row for row in collected_result if row.customer_id == "C001"][0]
        assert c001_record.full_name == "John,Doe"

class TestManualInterventions:
    
    @pytest.fixture(scope="class")
    def spark_session(self):
        spark = SparkSession.builder \
            .appName("ManualInterventionsTest") \
            .config("spark.sql.adaptive.enabled", "true") \
            .getOrCreate()
        yield spark
        spark.stop()
    
    def test_broadcast_join_optimization(self, spark_session):
        """TC006: Test broadcast join optimization"""
        # Create test data
        large_data = [(i, f"value_{i}") for i in range(1000)]
        small_data = [(i, f"lookup_{i}") for i in range(10)]
        
        large_df = spark_session.createDataFrame(large_data, ["id", "value"])
        small_df = spark_session.createDataFrame(small_data, ["id", "lookup"])
        
        # Test broadcast join
        result = large_df.join(broadcast(small_df), "id", "inner")
        
        assert result.count() == 10  # Only matching records
    
    def test_repartitioning_strategy(self, spark_session):
        """TC007: Test repartitioning strategy"""
        data = [(i, f"partition_{i % 4}", f"value_{i}") for i in range(100)]
        df = spark_session.createDataFrame(data, ["id", "partition_key", "value"])
        
        # Test repartitioning
        repartitioned_df = df.repartition("partition_key")
        
        # Verify repartitioning doesn't change data count
        assert repartitioned_df.count() == 100
    
    def test_udf_conversion(self, spark_session):
        """TC008: Test UDF conversion"""
        from pyspark.sql.functions import udf
        from pyspark.sql.types import StringType
        
        @udf(returnType=StringType())
        def custom_format(value):
            return f"formatted_{value}"
        
        data = [(1, "test1"), (2, "test2")]
        df = spark_session.createDataFrame(data, ["id", "value"])
        
        result = df.withColumn("formatted_value", custom_format("value"))
        collected_result = result.collect()
        
        assert collected_result[0].formatted_value == "formatted_test1"
    
    def test_error_handling(self, spark_session):
        """TC009: Test error handling"""
        data = [(1, "valid"), (2, None)]
        df = spark_session.createDataFrame(data, ["id", "value"])
        
        try:
            # Test handling of NULL values in operations
            result = df.filter(df.value.isNotNull())
            assert result.count() == 1
        except Exception as e:
            pytest.fail(f"Error handling failed: {str(e)}")
    
    def test_memory_optimization(self, spark_session):
        """TC010: Test memory optimization with caching"""
        data = [(i, f"value_{i}") for i in range(1000)]
        df = spark_session.createDataFrame(data, ["id", "value"])
        
        # Test caching
        cached_df = df.cache()
        
        # Multiple operations on cached DataFrame
        count1 = cached_df.count()
        count2 = cached_df.filter(cached_df.id > 500).count()
        
        assert count1 == 1000
        assert count2 == 499
        
        # Cleanup
        cached_df.unpersist()

if __name__ == "__main__":
    pytest.main(["-v", __file__])
```

### Performance Test Script

```python
import pytest
import time
from pyspark.sql import SparkSession
from pyspark.sql.functions import broadcast, col

class TestPerformanceOptimizations:
    
    @pytest.fixture(scope="class")
    def spark_session(self):
        spark = SparkSession.builder \
            .appName("PerformanceTest") \
            .config("spark.sql.adaptive.enabled", "true") \
            .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
            .getOrCreate()
        yield spark
        spark.stop()
    
    def test_broadcast_vs_regular_join_performance(self, spark_session):
        """Compare performance between broadcast and regular joins"""
        # Create test datasets
        large_data = [(i, f"large_value_{i}") for i in range(10000)]
        small_data = [(i, f"small_value_{i}") for i in range(100)]
        
        large_df = spark_session.createDataFrame(large_data, ["id", "large_value"])
        small_df = spark_session.createDataFrame(small_data, ["id", "small_value"])
        
        # Regular join timing
        start_time = time.time()
        regular_result = large_df.join(small_df, "id", "inner")
        regular_count = regular_result.count()
        regular_time = time.time() - start_time
        
        # Broadcast join timing
        start_time = time.time()
        broadcast_result = large_df.join(broadcast(small_df), "id", "inner")
        broadcast_count = broadcast_result.count()
        broadcast_time = time.time() - start_time
        
        # Verify results are identical
        assert regular_count == broadcast_count == 100
        
        # Log performance metrics (broadcast should be faster for small tables)
        print(f"Regular join time: {regular_time:.2f}s")
        print(f"Broadcast join time: {broadcast_time:.2f}s")
    
    def test_caching_performance(self, spark_session):
        """Test caching performance benefits"""
        data = [(i, f"value_{i}", i * 2) for i in range(5000)]
        df = spark_session.createDataFrame(data, ["id", "value", "amount"])
        
        # Without caching
        start_time = time.time()
        count1 = df.filter(col("amount") > 1000).count()
        count2 = df.filter(col("amount") < 5000).count()
        no_cache_time = time.time() - start_time
        
        # With caching
        cached_df = df.cache()
        start_time = time.time()
        cached_count1 = cached_df.filter(col("amount") > 1000).count()
        cached_count2 = cached_df.filter(col("amount") < 5000).count()
        cache_time = time.time() - start_time
        
        # Verify results are identical
        assert count1 == cached_count1
        assert count2 == cached_count2
        
        print(f"No cache time: {no_cache_time:.2f}s")
        print(f"With cache time: {cache_time:.2f}s")
        
        cached_df.unpersist()

if __name__ == "__main__":
    pytest.main(["-v", __file__])
```

## CONCLUSION

This conversion test case document provides comprehensive coverage for Hive to PySpark migration, including:

1. **Syntax Change Detection**: Detailed mapping of HiveQL constructs to PySpark equivalents
2. **Manual Intervention Guidelines**: Performance optimizations and edge case handling
3. **Test Case Coverage**: 10 comprehensive test cases covering all major conversion aspects
4. **Automated Testing**: Complete pytest scripts for validation

The test cases ensure data integrity, performance optimization, and functional equivalence between the original Hive procedures and converted PySpark code. Each test case includes specific validation criteria and expected outcomes to guarantee successful migration.

**Key Success Metrics:**
- All syntax conversions produce identical results
- Performance improvements through broadcast joins and caching
- Proper error handling and memory management
- Schema compatibility and data type integrity
- Comprehensive test coverage with automated validation